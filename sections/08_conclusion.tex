% Section 8: Conclusion
% Last updated from: v2_2025-01-12.md

\section{Conclusion}
\label{sec:conclusion}

High-stakes deployments of LLM-based systems demand more than \emph{transparent-seeming} narratives; they require explanations that can be \emph{challenged, audited, and revised}. Recent evidence suggests that post-hoc ``reasoning traces'' are often not a reliable proxy for what drives model behavior: when a prompt-injected hint changes a model's answer, state-of-the-art reasoning models reveal that hint in their chain-of-thought only about \textbf{25--39\%} of the time, indicating substantial unfaithfulness of verbalized rationales to causal drivers of outputs~\cite{chen2025reasoning}. This paper contributes (1) \textbf{\foam{}}, a pluralistic deliberation architecture for explainability-and-contestability-by-design; (2) an \textbf{inspectable provenance mechanism} that makes sentence-level claims traceable to source spans and contestable at the level stakeholders actually dispute; and (3) an \textbf{audit-style empirical evaluation} in evidence-grounded policy debate generation. In a double-blind tournament of 66 cases, the \foam{}-based system achieves higher overall scores than expert-human and zero-shot baselines (Table~\ref{tab:main-results}) and dramatically higher perfect evidence validation rates (Table~\ref{tab:validation}), demonstrating that accountable generation can be simultaneously \emph{high-quality} and \emph{verifiable}.

For the FAccT community, the central implication is a practical shift from explanation-as-disclosure to \textbf{contestable explanations}: outputs whose \emph{claims, warrants, and evidence links} are explicit, inspectable, and designed to invite targeted challenge (\eg disputing a cited sentence, contesting a warrant, or requesting an alternative perspective node). This orientation is consistent with due-process motivations for a meaningful right to contest consequential automated decisions~\cite{kaminski2021right}. Where governance requires reason-giving that can withstand scrutiny, pluralistic deliberation plus verifiable provenance offers a concrete design pattern for building AI systems whose decisions can be examined, contested, and improved without relying on ``black-box'' rationalizations.
