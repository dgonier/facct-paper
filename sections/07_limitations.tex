% Section 7: Limitations and Future Work
% Last updated from: v1_2025-01-12_baseline.md

\section{Limitations and future work}
\label{sec:limitations}

\subsection{Methodological limitations and validity threats}
\label{sec:methodological-limitations}

First, our primary outcome measure relies on an automated judge (Claude Opus 4) to score debate artifacts under a fixed rubric. While LLM-as-judge evaluation is increasingly standard at scale, it is known to exhibit systematic biases (\eg position effects, verbosity/style sensitivity, and self-enhancement tendencies) and may be vulnerable to prompt- or framing-based perturbations that shift preferences without corresponding semantic differences~\cite{zheng2023judging,shi2024judging,chen2024humans}. We reduce---but do not eliminate---these threats via double-blinding, standardized prompts, and by pairing judge scores with an independent evidence-validation audit. Nevertheless, the reported tournament results should be interpreted as descriptive for this evaluation setup, and future replications should triangulate across multiple judge models and human adjudication.

Second, our system's accountability guarantees are conditioned on the properties of the underlying evidence substrate. Sentence-level provenance constrains the model to point to specific source sentences rather than inventing citations, but it does not ensure that the retrieved evidence is complete, representative, or up to date. Coverage gaps, topical skew, and retrieval errors can shape which arguments are discoverable, and can yield outputs that are ``well-cited'' yet misleading due to selection effects, over-aggregation, or missing context~\cite{roush2025superpersuasive}. These concerns are not unique to debate generation: any contestability mechanism built on curated corpora inherits the corpus' blind spots. Accordingly, \foam{} should be viewed as an approach to making claims auditable and challengeable---not as a guarantee that the selected evidence is normatively ``best'' or epistemically sufficient.

Third, our evaluation scope is intentionally narrow and therefore limits external validity. We benchmark a specialized argumentative domain (policy debate) and a bounded artifact type (constructive case generation), and we do not yet measure downstream stakeholder contestation behaviors (\eg whether affected parties can efficiently detect, understand, and successfully challenge specific warrants or citations). Additionally, our ``perfect validation'' metric is strict by design: it favors verbatim traceability and can under-credit faithful paraphrase or correct claims supported by multiple dispersed sentences. Conversely, the metric may fail to detect other fidelity failures (\eg cherry-picked quoting or context stripping) that require richer contextual checks. These are appropriate trade-offs for an audit-style evaluation, but they motivate follow-on studies with complementary human-centered and context-sensitive validation protocols.

\subsection{Safety and misuse considerations}
\label{sec:safety-misuse}

Systems optimized for persuasive argumentation can be dual-use; we address misuse risks, affected groups, and mitigations in the Adverse Impacts statement (Endmatter).

\subsection{Future work}
\label{sec:future-work}

A first priority is human-subject evaluation of contestability as an interaction property rather than a static artifact property. We plan controlled studies in which participants (including domain experts and affected stakeholders) attempt to (i) locate supporting evidence for a contested sentence, (ii) challenge a warrant or inference step, and (iii) request or compare alternative perspective nodes. Primary outcomes should include time-to-challenge, challenge success rates, perceived procedural fairness, and the degree to which the system supports actionable revision pathways (\eg retracting a claim, swapping evidence, or surfacing counter-arguments) rather than merely producing longer explanations.

A second priority is extending \foam{} with optimization and training methods while preserving contestability constraints. Preliminary results in iterative preference learning suggest that tactic selection and evidence integration can be improved, but also reveal failure modes that matter for accountable deliberation. Future work should explore training objectives that explicitly reward faithful warrant-evidence alignment (not only persuasiveness) and contestation-aware curricula.
