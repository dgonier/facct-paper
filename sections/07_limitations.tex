% Section 7: Limitations and Future Work
% Last updated from: v1_2025-01-12_baseline.md

\section{Limitations and future work}
\label{sec:limitations}

\subsection{Methodological limitations and validity threats}
\label{sec:methodological-limitations}

First, our primary outcome measure relies on an automated judge (Claude Opus 4) to score debate artifacts under a fixed rubric. While LLM-as-judge evaluation is increasingly standard at scale, it is known to exhibit systematic biases (\eg position effects, verbosity/style sensitivity, and self-enhancement tendencies) and may be vulnerable to prompt- or framing-based perturbations that shift preferences without corresponding semantic differences~\cite{zheng2023judging,shi2024judging,chen2024humans}. We mitigate these threats through three design choices: (i) double-blinding (judge sees anonymized cases), (ii) model separation (generation uses Claude Haiku/Sonnet; judging uses Claude Opus 4, a different model that did not produce the outputs it evaluates), and (iii) pairing quality scores with an independent evidence-validation audit that does not depend on LLM judgment.

Nevertheless, models from the same provider may share systematic preferences (\eg favoring structured outputs, particular rhetorical patterns, or longer responses). The reported tournament results should be interpreted as descriptive for this evaluation setup. Future replications should triangulate across: (a) judge models from different providers (GPT-4, Gemini, open-source alternatives), (b) human expert adjudication on a representative subset, and (c) robustness analysis across rubric variations. We view the evidence validation results (Table~\ref{tab:validation}) as more robust than the quality scores (Table~\ref{tab:main-results}), since validation is computed deterministically without LLM judgment.

Second, our system's accountability guarantees are conditioned on the properties of the underlying evidence substrate. Sentence-level provenance constrains the model to point to specific source sentences rather than inventing citations, but it does not ensure that the retrieved evidence is complete, representative, or up to date. Coverage gaps, topical skew, and retrieval errors can shape which arguments are discoverable, and can yield outputs that are ``well-cited'' yet misleading due to selection effects, over-aggregation, or missing context~\cite{roush2025superpersuasive}. These concerns are not unique to debate generation: any contestability mechanism built on curated corpora inherits the corpus' blind spots. Accordingly, \foam{} should be viewed as an approach to making claims auditable and challengeable---not as a guarantee that the selected evidence is normatively ``best'' or epistemically sufficient.

Third, our evaluation measures verifiability rather than full contestability. A complete contestability evaluation would measure: (i) \textit{localization efficiency}---time/steps to identify a disputed premise in the mediation graph; (ii) \textit{challenge success rate}---whether targeted counterevidence triggers appropriate revision; (iii) \textit{revision locality}---how much structure changes to fix an identified issue; and (iv) \textit{perceived procedural fairness}---whether affected parties find the workflow comprehensible. We treat these as essential follow-on studies (Section~\ref{sec:future-work}).

Fourth, our evaluation scope is intentionally narrow and therefore limits external validity. We benchmark a specialized argumentative domain (policy debate) and a bounded artifact type (constructive case generation), and we do not yet measure downstream stakeholder contestation behaviors (\eg whether affected parties can efficiently detect, understand, and successfully challenge specific warrants or citations). Additionally, our CFVR metric is strict by design: it favors verbatim traceability and can under-credit faithful paraphrase or correct claims supported by multiple dispersed sentences. Conversely, the metric may fail to detect other fidelity failures (\eg cherry-picked quoting or context stripping) that require richer contextual checks. These are appropriate trade-offs for an audit-style evaluation, but they motivate follow-on studies with complementary human-centered and context-sensitive validation protocols.

Fifth, our evaluation embeds cultural assumptions that may limit generalizability. American competitive policy debate reflects particular adversarial norms---burden-shifting, time-constrained advocacy, winner-take-all adjudication---that do not map cleanly onto all contestation contexts. In settings where affected parties lack advocacy resources, adversarial framing may exacerbate power asymmetries. \foam{}'s architecture is agnostic to adversarial vs.\ collaborative deliberation; future work should explore instantiations in participatory governance settings (\eg citizen assemblies, collaborative sensemaking) where the goal is joint understanding rather than competitive victory.

\subsection{Safety and misuse considerations}
\label{sec:safety-misuse}

Systems optimized for persuasive argumentation can be dual-use; we address misuse risks, affected groups, and mitigations in the Adverse Impacts statement (Endmatter).

\subsection{Future work}
\label{sec:future-work}

A first priority is human-subject evaluation of contestability as an interaction property rather than a static artifact property. We plan controlled studies in which participants (including domain experts and affected stakeholders) attempt to (i) locate supporting evidence for a contested sentence, (ii) challenge a warrant or inference step, and (iii) request or compare alternative perspective nodes. Primary outcomes should include time-to-challenge, challenge success rates, perceived procedural fairness, and the degree to which the system supports actionable revision pathways (\eg retracting a claim, swapping evidence, or surfacing counter-arguments) rather than merely producing longer explanations.

A second priority is extending \foam{} with optimization and training methods while preserving contestability constraints. Preliminary results in iterative preference learning suggest that tactic selection and evidence integration can be improved, but also reveal failure modes that matter for accountable deliberation. Future work should explore training objectives that explicitly reward faithful warrant-evidence alignment (not only persuasiveness) and contestation-aware curricula.
