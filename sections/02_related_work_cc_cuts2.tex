% Section 2: Related Work
% Last updated from: v1_2025-01-12_baseline.md

\section{Accountability requirements and related work}
\label{sec:related-work}

\subsection{Explainability requirements beyond transparency}
\label{sec:explainability-requirements}

Contemporary calls for ``explainable AI'' often conflate \textbf{transparency} (exposing internal mechanisms) with \textbf{explanation} (providing reasons that are meaningful for a particular audience and purpose). Lipton argues that interpretability is not a single property and that many ``explanations'' in ML function as \emph{post-hoc rationalizations} whose relationship to actual model behavior is ambiguous, especially when the explanation's audience is a regulator, decision-subject, or domain expert rather than a model developer~\cite{lipton2018mythos}. Relatedly, Doshi-Velez \& Kim emphasize that interpretability claims must be made relative to \textbf{use context}---including the user's expertise, the stakes, and the kind of decision being supported---because what counts as a satisfactory explanation differs across settings~\cite{doshivelez2017towards}. In high-stakes domains, this motivates either (i) models that are inherently interpretable, or (ii) explanation mechanisms that achieve a comparable standard of \emph{reliability and auditability} rather than superficial plausibility~\cite{rudin2019stop}.

For accountability, explanations must be more than persuasive narratives; they must be \textbf{diagnostically useful} and \textbf{robust to strategic manipulation}. The NLP interpretability literature distinguishes \emph{plausibility} (does an explanation look reasonable?) from \emph{faithfulness} (does it track the true basis of the output?), arguing that faithful explanations require evaluation criteria beyond ``nice-sounding'' rationales~\cite{jacovi2020towards}. Explainability requirements in FAccT-relevant deployments should be stated in terms of \textbf{checkability}: tracing claims to concrete support and isolating points of disagreement~\cite{miller2019explanation,jacovi2020towards}.

\subsection{Contestability as a system property}
\label{sec:contestability}

Explainability alone does not guarantee that affected parties can meaningfully challenge an AI-mediated decision; contestability is best treated as a \textbf{system-level governance property} rather than an after-the-fact user interface feature. Alfrink et al.\ frame ``contestable AI by design'' as the view that systems should be built to \emph{support} contestation---through traceability, structured justification, and pathways for challenge---rather than treating contestation as an external legal or organizational process that happens ``around'' the model~\cite{alfrink2023contestable}. Legal scholarship on automated decision-making similarly emphasizes that accountability requires more than disclosure: decision-subjects need procedures to \emph{question, rebut, and obtain redress}, and these procedures depend on the availability of intelligible grounds and records of how outputs were produced~\cite{kroll2017accountable}. This is particularly important because the existence and scope of a freestanding ``right to explanation'' under the GDPR is contested, with influential analyses arguing that GDPR does not straightforwardly provide a general right to detailed model explanations---reinforcing the need for contestability mechanisms that do not rely on a single doctrinal reading of transparency rights~\cite{wachter2017right}.

Operationally, contestability implies three minimal requirements:
\begin{enumerate}
    \item \textbf{Visibility} that an automated or AI-assisted decision has occurred and can be challenged;
    \item \textbf{Comprehensibility} of the stated grounds and supporting materials; and
    \item \textbf{Actionability}, meaning a practical pathway to present counterevidence/counterarguments and obtain review and potential revision~\cite{alfrink2023contestable,kroll2017accountable}.
\end{enumerate}

Legal and governance frameworks reinforce this design target: GDPR Article 22 provisions and EU Trustworthy AI guidance treat accountability as including mechanisms for redress and challenge~\cite{eu2016gdpr,eu2019trustworthy}. This motivates \textbf{contestability as an end-to-end workflow} linking reasons to evidence and enabling structured challenge~\cite{raji2020closing,alfrink2023contestable}.

\subsection{Pluralistic and deliberative approaches to accountability}
\label{sec:pluralistic-approaches}

In high-stakes settings, disagreement is not merely empirical but normative. Feminist epistemology argues that knowledge claims are situated and ``view from nowhere'' objectivity can mask whose interests are operationalized~\cite{haraway1988situated}. For AI accountability, this motivates an architectural stance: systems should make \textbf{value trade-offs explicit} and preserve dissenting considerations that can be examined and contested~\cite{miller2019explanation,haraway1988situated}.

Recent work in value alignment and governance likewise emphasizes that ``alignment'' is underdetermined when stakeholders disagree about objectives, priorities, and acceptable risks. Kasirzadeh distinguishes forms of alignment that presume a single coherent value target from approaches that treat plural and conflicting values as first-class constraints---implying that accountability mechanisms must represent disagreement rather than suppress it~\cite{kasirzadeh2023conversation}. In parallel, ``society-in-the-loop'' framings argue that algorithmic systems are components of an evolving social contract and therefore require institutionalized interfaces for dispute, oversight, and revision~\cite{rahwan2018society}. In FAccT terms, these perspectives justify \textbf{pluralistic explanation}: not as an optional UX feature, but as a governance mechanism that helps stakeholders identify where the system's reasoning depends on contestable assumptions~\cite{rahwan2018society,kasirzadeh2023conversation}.

\subsection{Multi-agent deliberation and debate in AI}
\label{sec:multi-agent-deliberation}

A technical pathway to operationalizing pluralism is to replace monolithic generation with \textbf{structured multi-agent deliberation}, including debate-style protocols. In AI safety, ``debate'' was proposed as a scalable oversight mechanism in which adversarial argumentation can surface flaws or deception that a single system might otherwise hide~\cite{irving2018ai}. Subsequent theoretical work studies conditions under which debate can be made efficient and verifiable, strengthening the conceptual link between adversarial dialogue and reliable oversight~\cite{browncohen2024scalable}. Empirically, multi-agent debate among language models has been reported to improve factuality and reasoning in some settings, suggesting that disagreement and cross-examination can function as error-correction dynamics rather than mere rhetoric~\cite{du2023improving}. However, most ``LLM debate'' results are evaluated in terms of accuracy or judge preference; they do not, by themselves, guarantee that the resulting justifications are \textbf{auditable} or that third parties can meaningfully contest specific premises, evidence selections, or value judgments~\cite{rudin2019stop,jacovi2020towards}.

Computational argumentation provides complementary foundations for making deliberation outputs contestable because it supplies explicit representations of \textbf{claims, warrants, attacks, defenses, and (in value-based variants) normative priorities}. Toulmin's model remains foundational for analyzing argument structure in terms of claims supported by warrants and backing~\cite{toulmin1958uses}. Formal work in AI argumentation further develops abstract and assumption-based frameworks for representing defeasible reasoning, while value-based argumentation captures how outcomes change when different values are prioritized~\cite{benchcapon2009argumentation,toni2014tutorial}. Surveys connecting argumentation and XAI argue that these representations can support explanation as a structured object of inquiry---closer to an ``inspectable case'' than a narrative rationale---because stakeholders can contest particular premises or inference steps and observe how the conclusion changes~\cite{vassiliades2021argumentation}. This literature motivates the core related-work claim that a \emph{contestable} AI system should produce not only an answer, but also a \textbf{dispute-ready argumentative record}: reasons decomposed into contestable units, linked to supporting materials, and amenable to revision under challenge~\cite{kroll2017accountable,vassiliades2021argumentation}.
