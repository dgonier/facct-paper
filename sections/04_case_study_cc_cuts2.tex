% Section 4: Case Study
% Last updated from: v1_2025-01-12_baseline.md

\section{Case study system: evidence-grounded policy debate generation}
\label{sec:case-study}

\subsection{Why policy debate is an accountability crucible}
\label{sec:debate-crucible}

We instantiate \foam{} in a domain where \emph{contestability is native to the task}: American competitive policy debate. Policy debate is a two-team adversarial format in which teams argue for and against a policy proposal under strict procedural constraints~\cite{snider2008code}. In this ecosystem, argument quality is not evaluated purely as rhetorical fluency; instead, the activity is structured around \emph{traceable evidentiary support} and explicit clash, so claims can be challenged in real time and revisited across subsequent speeches. Critically, policy debate operationalizes ``grounding'' through an established evidence artifact: the \emph{debate card}. A card typically includes (i) a short biased summary intended to support a specific argumentative function, (ii) a full citation, and (iii) verbatim quoted source text, often with token-level highlighting that marks precisely what will be read into the round. Competitive success is strongly coupled to evidence quality and its deployment, creating an evaluation environment where provenance and verifiability are not optional.

\subsection{Pipeline overview}
\label{sec:pipeline-overview}

Figure~\ref{fig:pipeline} summarizes our \textbf{five-phase pipeline} for generating an evidence-grounded constructive speech (the 1AC, in our evaluation setting). Phases 1--3 produce an inspectable argumentative plan in typed components (perspective assignment $\rightarrow$ strategic plan $\rightarrow$ template traversal), Phase 4 binds each argumentative component to \emph{verbatim evidence at sentence granularity} (sentence-level provenance), and Phase 5 compiles and verifies the result (structural conformance, evidence/claim alignment, and perspective consistency). The key design principle is to keep the model in a role where it can be audited: rather than ``write a persuasive case and cite sources,'' the system decomposes ``case construction'' into a sequence of constrained decisions that leave a machine-checkable trail.

\begin{figure}[htbp]
\centering
\includegraphics[width=\columnwidth]{figures/contestability_mechanics.png}
\caption{Five-phase pipeline with accountability mechanisms. Phases 1--3 (Perspective Assignment, Plan Generation, Template Traversal) handle argumentative planning. Phase 4 (Evidence Binding) creates sentence-level provenance by selecting specific sentence IDs rather than paraphrasing. Phase 5 (Compilation) enforces verification checks. The output is a contestable speech artifact with claims, warrants, and traceable evidence links.}
\Description{Flow diagram showing five sequential phases: Phase 1 Perspective Assignment (with 24-dim perspective node output), Phase 2 Plan Generation (with coherence scoring and dialogical refinement), Phase 3 Template Traversal (with critic, evaluator, and typed syllogisms), Phase 4 Evidence Binding (with sentence indexing and sentence-level provenance highlighted in green as accountability mechanisms), and Phase 5 Compilation (with verification checks and provenance map). All phases flow to a final Contestable Speech Artifact output containing claims, warrants, and evidence links.}
\label{fig:pipeline}
\end{figure}

\subsection{Phases 1--3: perspective assignment, planning, and template traversal}
\label{sec:phases-1-3}

Phases 1--3 produce an inspectable argumentative plan through three contestability-relevant operations. In \textbf{Phase 1}, the system assigns an explicit perspective node (Section~\ref{sec:differentiated-agents}), making the evaluative frame a first-class auditable choice. In \textbf{Phase 2}, a dialectical refinement loop stress-tests the strategic plan: a Critic agent issues typed objections (logical gap, missing evidence, value conflict, scope overreach), an Evaluator scores each objection's materiality, and the Proposer revises or rebuts. This cycle iterates at least three times, and \emph{all objections---including dismissed ones---remain in the mediation graph}, enabling downstream reviewers to inspect whether a weakness was raised and why the response was deemed adequate.

In \textbf{Phase 3}, template tree traversal expands the plan into a typed syllogism scaffold (\eg Advantage = Uniqueness + Link + Impact). At each branch point, the system records which template was selected, what word allocation was applied (\eg 30\% impact, 40\% link), and whether novel templates were generated. This trace enables a distinct class of challenges: stakeholders can dispute not only \emph{what} claims were made, but \emph{why the argumentative structure took this form rather than another}---for instance, contesting that a utilitarian impact calculus was chosen when the underlying values favor a rights-based framing.

\subsection{Phase 4: sentence-level provenance}
\label{sec:phase-4}

\textbf{Motivation.} Retrieval-augmented generation can reduce hallucinations~\cite{lewis2020rag,shuster2021retrieval}, but it does not eliminate a central accountability failure mode: models may still produce claims that are \emph{unsupported by}, \emph{in conflict with}, or \emph{misattributed to} retrieved text. Recent benchmarks explicitly document that, even under RAG setups, LLM outputs can contain unsupported or contradictory content relative to the retrieved passages~\cite{gao2023rarr}. Phase 4 therefore implements a stronger constraint than ``retrieve then paraphrase'': it forces the model to operate over \emph{sentence identifiers} rather than free-form rewriting of source material.

\textbf{Mechanism.} Phase 4 is a two-step procedure:

\textbf{Step (a): sentence indexing.} The system queries a debate-evidence store (vector database of debate ``cards'') and segments retrieved documents into sentences with stable identifiers \texttt{(document\_id, sentence\_id)}.

\textbf{Step (b): evidence selection.} The LLM selects sentence IDs supporting each argument slot and generates short ``tags'' stating what each sentence establishes. The model never restates evidence; final content is assembled from retrieved sentences directly. This eliminates fabricated quotations by construction: the model can select wrong sentences but cannot invent ones not in the retrieved set.

\textbf{Contestability properties.} Sentence-level provenance changes contestation from ``argue about what the model meant'' to ``inspect what the model relied on.'' Stakeholders can challenge (i) \emph{relevance}, (ii) \emph{adequacy}, or (iii) \emph{selection bias}---each targeting a concrete sentence ID. This aligns with policy debate norms where quoted text is the unit of disputation.

\subsection{Phase 5: compilation and verification checks}
\label{sec:phase-5}

Phase 5 compiles the typed argument scaffold (Phase 3) and the evidence bindings (Phase 4) into a final speech artifact suitable for evaluation. Compilation preserves the provenance map: each substantive claim in the rendered speech remains traceable to one or more sentence IDs plus citation metadata. The system then runs verification checks that are directly tied to the accountability requirements:
\begin{enumerate}
    \item \textbf{Structural completeness} (template validators---\eg required components are present),
    \item \textbf{Evidence/claim alignment} (each slot has at least one bound sentence; missing bindings fail closed), and
    \item \textbf{Perspective consistency} (warrants and impacts do not contradict the declared perspective node from Phase 1).
\end{enumerate}

Figure~\ref{fig:pipeline} highlights where provenance is created (Phase 4) and where it is enforced (Phase 5).
