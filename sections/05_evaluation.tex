% Section 5: Evaluation
% Last updated from: v1_2025-01-12_baseline.md

\section{Empirical Evaluation}
\label{sec:evaluation}

\subsection{Research questions}
\label{sec:research-questions}

We evaluate \foam{}'s accountable-generation claims using an \emph{audit-style} design: we define explicit research questions, compare against salient baselines, and report both performance outcomes and traceability outcomes as first-class metrics. This approach aligns with established work on internal algorithmic auditing and emerging ``assurance audit'' perspectives, which emphasize that accountability requires not only outcome quality, but also artifacts and procedures that make decisions inspectable and challengeable~\cite{raji2020closing,lam2024assurance}.

We ask whether \foam{} improves:
\begin{itemize}
    \item \textbf{RQ1:} Quality/persuasiveness
    \item \textbf{RQ2:} Evidence verifiability
    \item \textbf{RQ3:} Whether gains are attributable to the accountability mechanisms rather than model strength
\end{itemize}

\subsection{Experimental design and baselines}
\label{sec:experimental-design}

\textbf{Task selection.} We evaluate in evidence-grounded policy debate generation because it combines (i) long-horizon argumentative planning, (ii) adversarial robustness expectations (arguments must survive challenge), and (iii) strict evidentiary norms (claims are conventionally supported with citations). In computational argumentation, even highly resourced systems have historically relied on constrained debate settings and bespoke pipelines; the Project Debater line of work illustrates both the ambition of debate as a benchmark and the practical need to structure and constrain the task for reliable evaluation~\cite{slonim2021autonomous}.

\textbf{Debate artifact.} We focus on the \textbf{first affirmative constructive (1AC)} as the most demanding generative unit in competitive policy debate: it must introduce a full strategic position (advantages/disadvantages/solvency framing), anticipate common lines of negative attack, and do so under tight length constraints while maintaining evidentiary support. This makes the 1AC a strong proxy for high-stakes accountable generation: arguments must be \emph{comprehensible}, \emph{internally coherent}, and \emph{traceable to evidence} to be meaningfully contestable.

\textbf{Corpus and baselines.} We ran a \textbf{double-blind tournament of 66 cases} drawn from three sources:
\begin{enumerate}
    \item \textbf{\foam{}-based structured system} (``DebaterHub Structured System,'' $n=22$), generated via differentiated perspectives, iterative dialectical refinement, typed syllogisms, and sentence-level provenance;
    \item \textbf{Human expert baseline} ($n=23$), sampled from prestigious debate camps (Dartmouth, Georgetown, Michigan, Emory); and
    \item \textbf{Zero-shot AI baseline} ($n=21$), produced by frontier models (Gemini/Claude/ChatGPT/Grok) using prompt engineering and web-research access but without debate-specific pluralistic architecture.
\end{enumerate}

\textbf{Evidence corpus for provenance.} \foam{}'s evidence retrieval and validation leverage a structured debate-evidence corpus derived from OpenDebateEvidence, which (as released) contains \textbf{3.5M+} competitive debate documents with metadata useful for downstream argument mining and citation~\cite{roush2024opendebate}. Operationally, our system queries a vector database of $\sim$85,000 curated ``cards'' plus any newly processed sources, and the generation pipeline preserves \emph{sentence-level identifiers} so that downstream reviewers can trace claims to exact supporting spans.

\subsection{Judging rubric and scoring}
\label{sec:judging-rubric}

\textbf{Tournament format and blinding.} All submissions were anonymized and assigned unique IDs (\eg \texttt{Case\_001}), and judging proceeded purely on content without revealing origin. Cases advanced through a modified Swiss-style bracket with double elimination, and pairings were balanced by strategic approach (\eg traditional policy vs.\ kritik) to reduce ``judge adaptation'' artifacts. Ties within a narrow score band triggered evidence validation as a tiebreaker, keeping accountability-relevant verifiability salient in advancement decisions.

\textbf{Rubric and judge.} A Claude Opus 4 judge evaluated each case on five weighted dimensions:
\begin{itemize}
    \item \textbf{Argumentation Strength} (25\%)
    \item \textbf{Evidence Quality} (25\%)
    \item \textbf{Strategic Coherence} (20\%)
    \item \textbf{Innovation} (15\%)
    \item \textbf{Competitive Viability} (15\%)
\end{itemize}

The rubric was designed to reward both argumentative competence and evidence-groundedness, while preserving enough structure for reproducibility.

\subsection{Evidence validation methodology}
\label{sec:evidence-validation}

\textbf{Why evidence validation is an accountability metric (not just ``anti-hallucination'').} In contestable systems, stakeholders must be able to \emph{locate} and \emph{evaluate} the grounds of a claim---especially where persuasive language can obscure weak or missing support. Audit frameworks similarly emphasize that assurance depends on traceable evidence artifacts rather than outcome plausibility alone~\cite{raji2020closing,lam2024assurance}. We therefore operationalize verifiability as a measurable property of each case's citations.

\textbf{Automated citation checks and categories.} Each citation was automatically checked against the referenced source (via URL or resolvable reference), and classified into one of four buckets: \textbf{exact match}, \textbf{partial match}, \textbf{paraphrase}, or \textbf{fabricated}. We summarize results primarily via \textbf{Perfect Validation}, a stringent metric that counts only \textbf{exact matches}---\ie the cited claim can be located verbatim in the referenced source span. This is intentionally conservative: Perfect Validation corresponds to the strongest form of contestability, where an affected party can directly inspect the cited text without interpretive debate about semantic similarity.

\textbf{How \foam{} changes the validation problem.} \foam{}'s sentence-level provenance changes citation validation from a semantic retrieval problem into a \emph{pointer integrity} problem: the model is never asked to reproduce source text, but instead selects sentence indices from retrieved documents and attaches them to specific argument components. This design greatly reduces degrees of freedom for fabrication and enables deterministic re-checking of a case's evidentiary backbone.

\subsection{Results}
\label{sec:results}

\textbf{Main tournament outcomes.} Table~\ref{tab:main-results} reports aggregate performance by source. The \foam{}-based system achieved the highest overall score (\textbf{81.7}) relative to human experts (\textbf{70.1}) and zero-shot AI (\textbf{50.6}). The largest gap appears in \textbf{Evidence Quality} (\textbf{86.7} vs.\ \textbf{56.9} vs.\ \textbf{27.1}), consistent with the claim that provenance-constrained generation shifts the system from persuasive-but-unreliable outputs toward persuasive-and-grounded outputs.

\begin{table}[htbp]
\caption{Tournament Results by Source}
\label{tab:main-results}
\centering
\begin{tabular}{lccc}
\toprule
\textbf{Metric} & \textbf{\foam{}} & \textbf{Human Expert} & \textbf{Zero-shot AI} \\
\midrule
Overall Score & 81.7 & 70.1 & 50.6 \\
Evidence Quality & 86.7 & 56.9 & 27.1 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Evidence validation and verifiability.} Table~\ref{tab:validation} reports Perfect Validation rates. \foam{} achieved \textbf{76.2\%} Perfect Validation, compared to \textbf{8.7\%} for the human expert baseline and \textbf{0\%} for zero-shot AI. This is the central accountability result: the \foam{} pipeline does not merely produce arguments that a judge model rates as ``good,'' but produces arguments whose evidentiary support can be mechanically verified at scale.

\begin{table}[htbp]
\caption{Perfect Validation Rates}
\label{tab:validation}
\centering
\begin{tabular}{lc}
\toprule
\textbf{Source} & \textbf{Perfect Validation (\%)} \\
\midrule
\foam{} System & 76.2 \\
Human Expert & 8.7 \\
Zero-shot AI & 0.0 \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[htbp]
\centering
\includegraphics[width=\columnwidth]{figures/combined_results.pdf}
\caption{Tournament results comparing \foam{}, human expert baselines, and zero-shot AI. (a) Overall and Evidence Quality scores. (b) Perfect Validation rates---the percentage of citations that exactly match source text. \foam{} achieves 76.2\% perfect validation vs.\ 8.7\% for human experts and 0\% for zero-shot AI.}
\Description{Two bar charts side by side. Left chart shows Overall Score and Evidence Quality for three sources: FOAM (81.7/86.7), Human Experts (70.1/56.9), and Zero-shot AI (50.6/27.1). Right chart shows Perfect Validation percentages: FOAM 76.2\%, Human Experts 8.7\%, Zero-shot AI 0\%.}
\label{fig:evaluation-results}
\end{figure}

\textbf{Interpreting what is doing the work.} Two mechanisms plausibly drive the observed gap: (i) \textbf{pluralistic deliberation} (multi-perspective critique and refinement) improves strategic coherence and argument coverage, while (ii) \textbf{sentence-level provenance} directly improves evidence integrity and sharply limits fabrication opportunities. Consistent with this interpretation, the tournament champion (\texttt{Case\_045}, ``Navy Underwater Exploration'') achieved \textbf{fidelity = 1.0} alongside a strong final-round score, indicating that high persuasive quality and high verifiability can co-occur under the \foam{} constraint regime.
