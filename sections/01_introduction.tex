% Section 1: Introduction
% Last updated from: v1_2025-01-12_baseline.md

\section{Introduction}
\label{sec:introduction}

\subsection{Accountability gap in high-stakes AI}
\label{sec:accountability-gap}

AI systems are now routinely embedded in high-stakes decision workflows---healthcare triage and documentation~\cite{obermeyer2019dissecting}, hiring and workplace management~\cite{raghavan2020mitigating}, credit and insurance~\cite{kozodoi2022fairness}, public benefits~\cite{eubanks2018automating}, and criminal-legal risk assessments~\cite{angwin2016machinebias}. In these settings, ``performance'' cannot be reduced to predictive accuracy or user satisfaction: when a system's output influences outcomes that materially affect people's rights, opportunities, or safety, \textbf{accountability requires (i) intelligible reasons and (ii) effective avenues to challenge and revise those reasons}. Yet most deployed AI remains organized around a monolithic model that produces a single authoritative output, with limited transparency into \emph{why} it said what it said and little procedural support for contesting it when it is wrong, biased, or normatively inappropriate.

This accountability gap has two tightly coupled dimensions. \textbf{Explainability} is often treated as a documentation problem---generate a rationale, a summary, or a list of features---rather than a \emph{reason-giving} problem grounded in the kinds of explanations different stakeholders actually need (\eg diagnostic vs.\ role-based explanations)~\cite{miller2019explanation,yao2024explanatory}. \textbf{Contestability}, meanwhile, is frequently bolted on as an afterthought (appeals processes, ``report a problem'' buttons, or generic feedback loops) rather than built into the architecture of reasoning itself. Meaningful contestability requires at least (a) visibility into decision logic, (b) comprehensibility for affected parties, and (c) actionable mechanisms for challenge and revision~\cite{alfrink2023contestable}. A system that cannot surface its operative assumptions, show its evidentiary basis, and support structured disagreement cannot plausibly satisfy these conditions---especially in domains where reasonable stakeholders legitimately disagree about values, tradeoffs, and acceptable risk.

\subsection{Why post-hoc ``explanations'' break: the faithfulness problem}
\label{sec:faithfulness-problem}

A central reason current explainability tooling struggles is that it frequently relies on \textbf{post-hoc self-explanation from the same model that produced the decision}. For large language models in particular, chain-of-thought and rationale-style explanations can be fluent and persuasive while remaining weakly coupled to what actually drove the output. Chen et al.\ benchmark state-of-the-art reasoning models and report low overall faithfulness scores---\eg \textbf{25\% for Claude 3.7 Sonnet and 39\% for DeepSeek R1} under their evaluation design---highlighting that models may omit or misrepresent key determinants of their answers even when explicitly prompted to ``show their work''~\cite{chen2025reasoning}. Related work similarly emphasizes that CoT can be misleading as an interpretability proxy, especially when users treat it as a reliable window into computation rather than a generated text artifact~\cite{turpin2023language}.

This ``faithfulness gap'' creates a direct accountability failure mode: if the explanation channel can drift from the decision channel, then transparency becomes performative---useful for persuasion, but unreliable for oversight, auditing, or recourse. In high-stakes contexts, that is not a subtle limitation; it is a design-level mismatch between what institutions need (verifiable reasons and traceable evidence) and what monolithic systems can robustly provide. The core implication is architectural: \textbf{if we want explanations that can support contestation, we need systems that can produce multiple, checkable reason-giving traces---not a single narrative generated by the same mechanism being explained.} This motivates pluralistic approaches that externalize disagreement, force explicit warrants, and attach provenance to claims so that challenges can target the actual moving parts of the reasoning.

\subsection{What we propose (FOAM) and what is new}
\label{sec:foam-intro}

This paper develops and evaluates \textbf{pluralistic AI systems} that operationalize explainability and contestability through \textbf{structured multi-agent deliberation} rather than post-hoc narration. We introduce \textbf{\foam{} (Framework for Openly Augmented Mediation)}, an architecture that treats accountable AI outputs as the product of a mediated process:
\begin{enumerate}
    \item \textbf{Differentiated agents} with distinct roles and epistemic commitments (\eg advocate, skeptic, evidence-checker, values/impact assessor),
    \item \textbf{Deliberative protocols} that require agents to advance and respond to claims under explicit constraints (\eg argument typing, cross-examination, and structured rebuttal), and
    \item \textbf{Sublation operators}---formal mechanisms for preserving what survives critique while revising what fails, so that the system's final output is not merely an average of perspectives but a documented transformation through contestation.
\end{enumerate}

The intended artifact is not just a recommendation, but a contestable record: claims, counterclaims, evidentiary supports, explicit points of disagreement, and the rationale for any resolution.

We make three contributions:
\begin{enumerate}
    \item \textbf{Framework:} we provide a unified account of explainability \emph{and} contestability as a single design target, arguing that they should be treated jointly and realized through pluralistic mediation rather than monolithic self-report.
    \item \textbf{Architecture and mechanisms:} we formalize \foam{} as an implementable blueprint---agents, protocols, and revision operators---paired with provenance-oriented design choices that make challenges actionable (\eg grounding claims in checkable evidence rather than free-form summarization).
    \item \textbf{Empirical validation:} we report results from an evaluation of pluralistic debate generation in a double-blind tournament of \textbf{66 policy debate cases}, where our structured multi-agent system achieved an overall score of \textbf{81.7} compared to \textbf{70.1} for human experts and \textbf{50.6} for zero-shot AI, while also achieving \textbf{76.2\%} perfect evidence validation compared to \textbf{8.7\%} for human experts and \textbf{0\%} for unstructured AI---demonstrating that pluralistic architectures can produce outputs that are simultaneously more persuasive \emph{and} more verifiable in an adversarial, evidence-sensitive setting.
\end{enumerate}

We close by discussing implications for AI governance and by outlining a research agenda for \textbf{contestable AI by design}.
