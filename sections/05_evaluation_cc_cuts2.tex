% Section 5: Evaluation
% Last updated from: v1_2025-01-12_baseline.md

\section{Empirical Evaluation}
\label{sec:evaluation}

\subsection{Research questions}
\label{sec:research-questions}

We evaluate \foam{}'s accountable-generation claims using an \emph{audit-style} design: we define explicit research questions, compare against salient baselines, and report both performance outcomes and traceability outcomes as first-class metrics. This approach aligns with established work on internal algorithmic auditing and emerging ``assurance audit'' perspectives, which emphasize that accountability requires not only outcome quality, but also artifacts and procedures that make decisions inspectable and challengeable~\cite{raji2020closing,lam2024assurance}.

We ask whether \foam{} improves:
\begin{itemize}
    \item \textbf{RQ1:} Quality/persuasiveness under adversarial evaluation
    \item \textbf{RQ2:} Evidence verifiability---a necessary precondition for contestability
    \item \textbf{RQ3:} Whether gains are attributable to the accountability mechanisms rather than model strength or corpus advantages
\end{itemize}

\textbf{Scope of evaluation.} We evaluate verifiability rather than end-to-end contestability because the latter requires human-subject studies of challenge behaviors (time-to-locate-disputed-premise, challenge success rates, revision outcomes), which we scope as future work (Section~\ref{sec:future-work}). However, policy debate provides partial ecological validity: arguments that cannot survive adversarial cross-examination are systematically punished, so tournament success functions as a domain-native stress test for whether outputs can withstand structured challenge.

We acknowledge that RQ3 is only partially addressed: while we control for prompt engineering and evidence access in baselines, we do not isolate contributions of (i) pluralistic deliberation vs (ii) sentence-level provenance vs (iii) template structure. We discuss this limitation and outline ablation designs in Section~\ref{sec:limitations}.

\subsection{Experimental design and baselines}
\label{sec:experimental-design}

\textbf{Task selection.} We evaluate in policy debate generation because it combines long-horizon argumentative planning, adversarial robustness expectations, and strict evidentiary norms~\cite{slonim2021autonomous}.

\textbf{Debate artifact.} We focus on the \textbf{first affirmative constructive (1AC)}, the most demanding generative unit: it must introduce a full strategic position under tight length constraints while maintaining evidentiary support---a strong proxy for high-stakes accountable generation.

\textbf{Corpus and baselines.} We ran a \textbf{double-blind tournament of 66 cases} drawn from three sources:
\begin{enumerate}
    \item \textbf{\foam{}-based structured system} ($n=22$), generated via differentiated perspectives, iterative dialectical refinement, typed syllogisms, and sentence-level provenance;
    \item \textbf{Human expert baseline} ($n=23$), sampled from expert-authored training materials from highly competitive policy debate programs; and
    \item \textbf{Zero-shot AI baseline} ($n=21$), produced by frontier models (Gemini/Claude/ChatGPT/Grok) using prompt engineering and web-research access but without debate-specific pluralistic architecture.
\end{enumerate}

\textbf{Baseline controls (zero-shot AI).} We generated baselines using Claude 4.5, GPT-5, SuperGrok Heavy, and Gemini 2.5 in research modes with a standardized ``mega-prompt'' enforcing the same 1AC conventions: \textbf{1300--1700 words}, debate formatting, fixed advantage/solvency structure, and a strict \textbf{no-fabrication policy} requiring models to mark uncertainty as \textbf{[EVIDENCE NEEDED]}. Unlike FOAM, baselines did not use multi-agent deliberation, typed syllogisms, or sentence-level provenance; citations remained unconstrained and were evaluated under the same validation pipeline. We generated one case per topic per condition and used outputs as-is.

\textbf{Evidence retrieval.} \foam{} combines multi-hop web research with OpenDebateEvidence lookup~\cite{roush2024opendebate}, selecting best-fit evidence per argument slot; sentence-level IDs enable traceability.

\textbf{Model separation.} To mitigate self-enhancement bias, generation uses Claude Haiku/Sonnet while judging uses Claude Opus 4. We acknowledge same-provider models may share preferences; Section~\ref{sec:limitations} discusses this limitation.

\subsection{Judging rubric and scoring}
\label{sec:judging-rubric}

\textbf{Tournament format.} All 66 submissions were anonymized and evaluated in a double-blind tournament (Appendix~\ref{sec:appendix-evaluation}). Tables~\ref{tab:main-results}--\ref{tab:validation} report aggregate statistics over the full corpus.

\textbf{Rubric and judge.} Following LLM-as-judge methodology~\cite{zheng2023judging}, a Claude Opus 4 judge evaluated each case on five weighted dimensions (Argumentation 25\%, Evidence 25\%, Coherence 20\%, Innovation 15\%, Viability 15\%; full rubric in Appendix~\ref{sec:appendix-evaluation}).

\subsection{Evidence validation methodology}
\label{sec:evidence-validation}

\textbf{Why evidence validation matters.} In contestable systems, stakeholders must locate and evaluate claim grounds. Audit frameworks emphasize that assurance depends on traceable evidence artifacts~\cite{raji2020closing,lam2024assurance}. We operationalize verifiability as a measurable property of citations.

\textbf{Automated citation checks.} Each citation was classified as exact match, partial match, paraphrase, or fabricated based on source verification (classification criteria in Appendix~\ref{sec:appendix-provenance}).

We report two complementary metrics:
\begin{itemize}
    \item \textbf{Citation-Level Exact Match Rate (CEMR):} The proportion of individual citations achieving exact or partial match status. This measures per-citation fidelity.
    \item \textbf{Case-Level Full Validation Rate (CFVR):} The proportion of cases where \textit{all} citations achieve exact or partial match. This measures whether an entire artifact is audit-ready.
\end{itemize}
Table~\ref{tab:validation} reports CFVR (the stricter, case-level metric). The key finding holds under both metrics: \foam{} dramatically outperforms baselines on evidence integrity.

\textbf{Validation procedure.} For \foam{} outputs, validation is near-trivial pointer integrity: the system assembles evidence from retrieved sentences by ID, so verification reduces to checking that the assembled text matches the indexed source at that ID. For baseline outputs, validation requires resolving citations to external sources (URLs, bibliographic references) and performing substring matching. This asymmetry partly explains \foam{}'s advantage: the architecture guarantees source accessibility by construction, whereas baselines may cite sources that are paywalled, non-digitized, or incompletely referenced.

\subsection{Results}
\label{sec:results}

\textbf{Main tournament outcomes.} Table~\ref{tab:main-results} reports aggregate performance by source. The \foam{}-based system achieved the highest overall score (\textbf{73.5}) relative to human experts (\textbf{62.4}) and zero-shot AI (\textbf{46.3}). The largest gap appears in \textbf{Evidence Quality} (\textbf{78.3} vs.\ \textbf{51.4} vs.\ \textbf{20.5}), consistent with the claim that provenance-constrained generation shifts the system from persuasive-but-unreliable outputs toward persuasive-and-grounded outputs.

\begin{table}[htbp]
\caption{Tournament Results by Source}
\label{tab:main-results}
\centering
\begin{tabular}{lccc}
\toprule
\textbf{Metric} & \textbf{\foam{}} & \textbf{Human Expert} & \textbf{Zero-shot AI} \\
\midrule
Overall Score & 73.5 & 62.4 & 46.3 \\
Evidence Quality & 78.3 & 51.4 & 20.5 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Evidence validation and verifiability.} Table~\ref{tab:validation} reports Case-Level Full Validation Rates (CFVR). \foam{} achieved \textbf{76.2\%} CFVR, compared to \textbf{8.7\%} for the human expert baseline and \textbf{0\%} for zero-shot AI. This is the central accountability result: the \foam{} pipeline does not merely produce arguments that a judge model rates as ``good,'' but produces arguments whose evidentiary support can be mechanically verified at scale.

\begin{table}[htbp]
\caption{Case-Level Full Validation Rate (CFVR): percentage of cases where ALL citations achieve exact or partial match.}
\label{tab:validation}
\centering
\begin{tabular}{lc}
\toprule
\textbf{Source} & \textbf{CFVR (\%)} \\
\midrule
\foam{} System & 76.2 \\
Human Expert & 8.7 \\
Zero-shot AI & 0.0 \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[htbp]
\centering
\includegraphics[width=\columnwidth]{figures/combined_results.pdf}
\caption{Tournament results comparing \foam{}, human expert baselines, and zero-shot AI. (a) Overall and Evidence Quality scores. (b) Case-Level Full Validation Rate (CFVR)---the percentage of cases where all citations achieve exact or partial match with source text. \foam{} achieves 76.2\% CFVR vs.\ 8.7\% for human experts and 0\% for zero-shot AI.}
\Description{Two bar charts side by side. Left chart shows Overall Score and Evidence Quality for three sources: FOAM (73.5/78.3), Human Experts (62.4/51.4), and Zero-shot AI (46.3/20.5). Right chart shows Perfect Validation percentages: FOAM 76.2\%, Human Experts 8.7\%, Zero-shot AI 0\%.}
\label{fig:evaluation-results}
\end{figure}

\textbf{Interpreting what is doing the work.} Two mechanisms plausibly drive the observed gap: (i) \textbf{pluralistic deliberation} (multi-perspective critique and refinement) improves strategic coherence and argument coverage, while (ii) \textbf{sentence-level provenance} directly improves evidence integrity and sharply limits fabrication opportunities. Several high-scoring FOAM cases achieved perfect validation (fidelity = 1.0), indicating that high persuasive quality and high verifiability can co-occur under the \foam{} constraint regime.
